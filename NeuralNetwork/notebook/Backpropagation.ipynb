{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Back Propagation\n",
    "Back propagation is essentially iterating through every single weight and adjusting the weight . After we are given a probability score from forward propagation, we use a cost function to calculate the the difference between our predicted and fitted values. Based on the summation difference of errors, we perform gradient descent and adjust the weights in the network accordingly.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "$ \\dfrac{\\delta E_{total}}{\\delta W} $\n",
    "\n",
    "Summation squared of errors. While this provide the difference of our predicted and fitted values, we need to figure our how to minimize the error. One solution is by performing numerical gradient estimation, where we take the smallest value from our equation (need to word better). The problem with this is that, when your neural network becomes more complex and you need a 3D plane, iterating to find the smallest value is slow. This is also prone to suffering from the non-convex problem, which is when we accidentally find the local minima instead of our global one. However, this problem can be esaily optimized....\n",
    "$$\\sum\\limits_{} .5 * (y - \\hat{y})^2 $$\n",
    "\n",
    "By taking the partial derivative of summation of errors, this will quickly help find the global minima for our cost function by providing us the rate of change of J in respect to the W (weights). By using a negative slope from our equation, it will help us find the minima of our cost function.\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = \\dfrac{\\delta J \\sum\\limits_{} .5 * (y - \\hat{y})^2}{\\delta W_{(2)}}$$\n",
    "\n",
    "So, we don't exactly want to take the partial derivative of a summation. When we use the sum rule in differentiation, we can move the summation outside the partial derivative.\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = \\dfrac{\\sum\\limits_{} \\delta J .5 * (y - \\hat{y})^2}{\\delta W_{(2)}}$$\n",
    "\n",
    "Lets remove the summation all together!\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = \\dfrac{\\delta J .5 * (y - \\hat{y})^2}{\\delta W_{(2)}}$$\n",
    "\n",
    "Product rule to get rid of the 1/2:\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = (y - \\hat{y})$$\n",
    "\n",
    "Now, lets's perform the chain rule. Y is our inputs, this is our constant and will be zero. $ \\hat{y} $ is our fitted equation with our activation function, weights, and inputs. Let's break that down with chain rule.\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = (y - \\hat{y}) * -(\\dfrac{\\delta \\hat{y}}{\\delta W_{(2)}}) $$\n",
    "\n",
    "$$\\dfrac{\\delta J}{\\delta W_{(2)}} = -(y - \\hat{y})(\\dfrac{\\delta \\hat{y}}{\\delta W_{(2)}})$$\n",
    "\n",
    "$ \\hat{y} $ is our total weights combined with our activation function f: $ \\hat{y} = f(z_{(3)}) $.\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = -(y - \\hat{y}) \\dfrac{\\delta \\hat{y}}{\\delta z_{(3)}} \\dfrac{\\delta z_{3}}{\\delta W_{(2)}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
