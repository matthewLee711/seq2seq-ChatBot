{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "Importance of understanding neural networks:\n",
    "\n",
    "Basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training/test sets, activation functions/what they do, softmax\n",
    "\n",
    "What are recurrent neural networks:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can RNN do that ANN cannot?\n",
    "\n",
    "Image captioning, language translation, sentiment classification, predictive typing, video classification, nlp, speech recognition, etc.\n",
    "\n",
    "Feed forward NN are strong global function approximators. In other words, you can have a very difficult classification function and the FFNN can figure out the generaltivity(lol) of it. Recurrent neural networks take this to another level and instead they cam compute/describe an entire program. They can almost be considered turing complete (system in which a program can be used to solve any computation problem).\n",
    "\n",
    "- ANN cannot deal with sequential or temporal data (because of weighted matrix and fixed input/output size)\n",
    "    - For example if a neural network is to output a caption for a video, a list of words in a specific order would be required. This is a sequence, which it cannot output due to the fact nn cannot have variate the number of node output. However if one word or three words in a non sequential order were required to describe the video, NN would be fine.\n",
    "    - Sequential is also not possible because when you are training a network, each feed forward iteration will have to depende\n",
    "- ANN lack memory (Cannot store past results)\n",
    "    - \n",
    "- ANN have a fixed architecture (Have to change the nn and re-train)\n",
    "    - There is a fixed number of processing steps (bc number of hidden layers is a hyper parameter\n",
    "    - Each neuron in is almost like an entire layer in an ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The flaws with RNN and why we need to use LSTM\n",
    "\n",
    "In theroy, RNNs seem like an awesome solution. However, when your RNN starts to become very deep and issue called \"vanishing gradient\" arises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red = input, blue = hidden neuron, green = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-one, one-to-many, many-to-one, many-to-many\n",
    "\n",
    "#### Many to one\n",
    "input sentence and return whether it was positive or not\n",
    "\n",
    "#### Many to Many\n",
    "The idea behind this is combining multiple anns together. You can combine a CNN with a RNN to create image captioning. Example. two people in a photo. The cnn will identify there are two. People will come from the rnn because it is functionally dependent on the second hidden state. In other words, given the word two, people should be next based on the RNN experience from training the initial image we inputted. EVery outputted word is dependent on the previous word LCRN. \n",
    "\n",
    "RNNs aren’t magic; they only work because trained networks identified and learned patterns in data during training time that they now look for during prediction.\n",
    "\n",
    "What makes RNN so exciting is that they allow for operation over sequences of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-75b952b1c7de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mrnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# x is an input vector, y is the RNN's output vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "# step function in Vanilla rnn\n",
    "\n",
    "class RNN:\n",
    "  def step(self, x):\n",
    "    # update the hidden state\n",
    "    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "    # compute the output vector\n",
    "    y = np.dot(self.W_hy, self.h)\n",
    "    return y\n",
    "\n",
    "rnn = RNN()\n",
    "y = rnn.step(x) # x is an input vector, y is the RNN's output vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Equation\n",
    "$$ h_t = \\tanh ( W_{hh} h_{t-1} + W_{xh} x_t ) $$\n",
    "\n",
    "RNN works awesome with stacking.\n",
    "One RNN is recieving input vectors and the second RNN is receiving the output of the first RNN as its input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 512 RNN units = 1RNN neuron that outputs a 512 wide vector -> A vector with 512 values.\n",
    "- One RNN unit -> an RNN with one hidden layer. Thus people say \"Stacking RNNs on top of each other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNN Math\n",
    "\n",
    "If an input or output neuron has a value at timestep t, we denote this vector as:\n",
    "input -> $ x_t $ output -> $y_t $\n",
    "\n",
    "Since we can have multiple hidden layers, we denote the hidden state vector at timestep t and hidden layer l as:\n",
    "hidden -> $ H_t^l $\n",
    "\n",
    "Example: Many-to-many RNN with sequential input, sequential output, multiple timesteps, and multiple hidden layers.\n",
    "$$\n",
    "h_t^l =\n",
    "\\begin{cases}\n",
    "f_w(h_{t-1}^l, x_t)  & \\text{for l = 1} \\\\\n",
    "f_w(h_{t-1}^l, h_t^{l-1})  & \\text{for l > 1}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s list out the possible functional dependencies for a given hidden state, based on the arrows and flow of information in the diagram:\n",
    "- An input\n",
    "- Hidden state at the previous timestep, same layer\n",
    "- Hidden state at the current timestep, previous layer\n",
    "A hidden state can have two functional dependencies at max. Just by looking at the diagram, the only impossible combination is to be dependent on both the input and a hidden state at the current timestep but previous layer. This is because the only hidden states that are dependent on input exist in the first hidden layer, where no such previous layer exists.\n",
    "\n",
    "Because of the impossible combination, we have to define two separate equations. An equation for the hidden state at hidden layer **1** and for layers after 1.\n",
    "\n",
    "The function **$ f_w $** computes the numeric hidden state vector for timestep **t** and layer **l**. This contains the activation function like in ANNs. **W** are the weights of the RNN and thus **f** is conditioned on **W**.\n",
    "\n",
    "\n",
    "You might notice that we have a couple issues:\n",
    "- When t = 1 — that is, when each neuron is at the initial timestep — then no previous timestep exists. However, we still attempt to pass h_0 as a parameter to ƒw.\n",
    "- If no input exists at time t — thus, x_t does not exist — then we still attempt to pass x_t as a parameter.\n",
    "\n",
    "Our respective solutions follow:\n",
    "- Define h_0 for any layer as 0\n",
    "- Consider x_t where no input exists at timestep t as 0\n",
    "\n",
    "\n",
    "5 different types of weight matrices:\n",
    "- input to hidden -> $W_{xh}$ < this maps an inpput vector **x** to hidden state vector h\n",
    "- hidden to hidden in time -> $W_{hht}^l$ < maps a hidden state vector **h** to another hidden state vector h along with time axis EX. $h_{t-1}$ to $h_t$ \n",
    "- hidden to hidden in depth -> $W_{hhd}^l$ < maps hidden state vector **h** to another hidden state vector h along the depth axis. EX. $h^{l-1}_t$ to $h^l_t$ \n",
    "- hidden to output -> $W_{hy}$ < maps hidden state vector **h** to an output vector **y**\n",
    "- biases -> $b_{h}^l, b_{y}^l$ < like ANN we add a constant bias vector that can vertically shift what we pass to the activation function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the function **fw**\n",
    "\n",
    "$$ h_t^l = f_w(h_t-1^l, x_t) \\text{ for L = 1} $$\n",
    "\n",
    "$$ = tanh($W_{hht}^l h_t-1^l + W_{xh}x_t + b_{h}^l) $$\n",
    "\n",
    "\n",
    "$$ h_t^l = f_w(h_{t-1}^l, h_t^{l-1}) \\text{ for L > 1} $$\n",
    "\n",
    "$$ = tanh($W_{hht}^l h_t-1^l + W_{xh}x_t + b_{h}^l) $$\n",
    "\n",
    "Does this look similar the ANN hidden function? It applies the weights to the corresponding parameters, adds the bias, and passes the weighted sum through an activation function to introduce non-linearities (aka raw probabilities). This contarsts from ANNs because RNNs operate over vectors versus scalars.\n",
    "\n",
    "We tend to use tanh with RNNs mostly because of their role in LSTMs. (Product graidents with a greater range and that htier second derviative don't die off as quickly. Tanh has a greater range than the sigmoid. y = -1 instead of y = 0, intercept the y-axis at y = 0 instead of y = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The final equation!\n",
    "Mapping hidden state to an output\n",
    "$$ y_t = W_{hy}h_l^t + b_y $$\n",
    "\n",
    "Depending on the context, we might need to remove the bias vector and apply a non-linearity like sigmoid (if need output to be a probability distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example\n",
    "One to many single layer rnn needs to output \"hello\"\n",
    "\n",
    "The NN has the vocabulary h,e,l,o. It only knows these four characters; exactly enough to produce the word \"hello\". We will input the first character \"h\" and from there expect the output at the following timesetps to be: \"e\", \"l\", \"l\", \"o\".\n",
    "\n",
    "Lets represent the input and output via one hot encoding, where each char is a vector with a 1 at the corresponding character position. since our vocabulary is [h,e,l,o], we can represent characters using a vector with four values.\n",
    "\n",
    "h=\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "e = \n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "l =\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "    0 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "o = \n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    0 \\\\\n",
    "    1 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We input the first letter and the word is complete. OR we have 4 inputs and 4 outputs. We sample the output at each timestep and feed it into th next as input. RNNs need to have a start and end token. They signify when the input begins and the output ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation with RNNs\n",
    "BPTT - back propagation through time.\n",
    "Article on vanishing gradient problem: https://ayearofai.com/rohan-4-the-vanishing-gradient-problem-ec68f76ffb9b\n",
    "\n",
    "Because of this RNNs kind of suck. So onto LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
