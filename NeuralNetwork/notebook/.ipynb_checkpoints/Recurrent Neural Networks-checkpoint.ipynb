{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent neural networks\n",
    "\n",
    "Importance of understanding neural networks:\n",
    "\n",
    "Basics of machine learning, linear algebra, neural network architecture, cost functions, optimization methods, training/test sets, activation functions/what they do, softmax\n",
    "\n",
    "What are recurrent neural networks:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What can RNN do that ANN cannot?\n",
    "\n",
    "Image captioning, language translation, sentiment classification, predictive typing, video classification, nlp, speech recognition, etc.\n",
    "\n",
    "Feed forward NN are strong global function approximators. In other words, you can have a very difficult classification function and the FFNN can figure out the generaltivity(lol) of it. Recurrent neural networks take this to another level and instead they cam compute/describe an entire program. They can almost be considered turing complete (system in which a program can be used to solve any computation problem).\n",
    "\n",
    "- ANN cannot deal with sequential or temporal data (because of weighted matrix and fixed input/output size)\n",
    "    - For example if a neural network is to output a caption for a video, a list of words in a specific order would be required. This is a sequence, which it cannot output due to the fact nn cannot have variate the number of node output. However if one word or three words in a non sequential order were required to describe the video, NN would be fine.\n",
    "    - Sequential is also not possible because when you are training a network, each feed forward iteration will have to depende\n",
    "- ANN lack memory (Cannot store past results)\n",
    "    - \n",
    "- ANN have a fixed architecture (Have to change the nn and re-train)\n",
    "    - There is a fixed number of processing steps (bc number of hidden layers is a hyper parameter\n",
    "    - Each neuron in is almost like an entire layer in an ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The flaws with RNN and why we need to use LSTM\n",
    "\n",
    "In theroy, RNNs seem like an awesome solution. However, when your RNN starts to become very deep and issue called \"vanishing gradient\" arises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Red = input, blue = hidden neuron, green = output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-to-one, one-to-many, many-to-one, many-to-many\n",
    "\n",
    "#### Many to one\n",
    "input sentence and return whether it was positive or not\n",
    "\n",
    "#### Many to Many\n",
    "The idea behind this is combining multiple anns together. You can combine a CNN with a RNN to create image captioning. Example. two people in a photo. The cnn will identify there are two. People will come from the rnn because it is functionally dependent on the second hidden state. In other words, given the word two, people should be next based on the RNN experience from training the initial image we inputted. EVery outputted word is dependent on the previous word LCRN. \n",
    "\n",
    "RNNs arenâ€™t magic; they only work because trained networks identified and learned patterns in data during training time that they now look for during prediction.\n",
    "\n",
    "What makes RNN so exciting is that they allow for operation over sequences of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# step function in Vanilla rnn\n",
    "\n",
    "class RNN:\n",
    "  def step(self, x):\n",
    "    # update the hidden state\n",
    "    self.h = np.tanh(np.dot(self.W_hh, self.h) + np.dot(self.W_xh, x))\n",
    "    # compute the output vector\n",
    "    y = np.dot(self.W_hy, self.h)\n",
    "    return y\n",
    "\n",
    "rnn = RNN()\n",
    "y = rnn.step(x) # x is an input vector, y is the RNN's output vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step Equation\n",
    "$$ h_t = \\tanh ( W_{hh} h_{t-1} + W_{xh} x_t ) $$\n",
    "\n",
    "RNN works awesome with stacking.\n",
    "One RNN is recieving input vectors and the second RNN is receiving the output of the first RNN as its input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 512 RNN units = 1RNN neuron that outputs a 512 wide vector -> A vector with 512 values.\n",
    "- One RNN unit -> an RNN with one hidden layer. Thus people say \"Stacking RNNs on top of each other\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla RNN Math\n",
    "\n",
    "If an input or output neuron has a value at timestep t, we denote this vector as:\n",
    "input -> $ x_t $ output -> $y_t $\n",
    "\n",
    "Since we can have multiple hidden layers, we denote the hidden state vector at timestep t and hidden layer l as:\n",
    "hidden -> $ H_t^l $\n",
    "\n",
    "Example: Many-to-many RNN has sequential input, sequential output, multiple timesteps, and multiple hidden layers.\n",
    "$$\n",
    "h_t^l =\n",
    "\\begin{cases}\n",
    "f_w(h_t-1^l, x_t)  & \\text{for l = 1} \\\\\n",
    "f_w(h_t-1^l, h_t^l-1)  & \\text{for l > 1}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
