{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Back Propagation\n",
    "Back propagation is essentially iterating through every single weight and adjusting the weight . After we are given a probability score from forward propagation, we use a cost function to calculate the the difference between our predicted and fitted values. Based on the summation difference of errors, we perform gradient descent and adjust the weights in the network accordingly.\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "$ \\dfrac{\\delta E_{total}}{\\delta W} $\n",
    "\n",
    "Summation squared of errors. While this provide the difference of our predicted and fitted values, we need to figure our how to minimize the error. One solution is by performing numerical gradient estimation, where we take the smallest value from our equation (need to word better). The problem with this is that, when your neural network becomes more complex and you need a 3D plane, iterating to find the smallest value is slow. This is also prone to suffering from the non-convex problem, which is when we accidentally find the local minima instead of our global one. However, this problem can be esaily optimized....\n",
    "$$\\sum\\limits_{} .5 * (y - \\hat{y})^2 $$\n",
    "\n",
    "By taking the partial derivative of summation of errors, this will quickly help find the global minima for our cost function by providing us the rate of change of J in respect to the W (weights). By using a negative slope from our equation, it will help us find the minima of our cost function.\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = \\dfrac{\\delta J \\sum\\limits_{} .5 * (y - \\hat{y})^2}{\\delta W_{(2)}}$$\n",
    "\n",
    "So, we don't exactly want to take the partial derivative of a summation. When we use the sum rule in differentiation, we can move the summation outside the partial derivative.\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = \\dfrac{\\sum\\limits_{} \\delta J .5 * (y - \\hat{y})^2}{\\delta W_{(2)}}$$\n",
    "\n",
    "Lets remove the summation all together!\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = \\dfrac{\\delta J .5 * (y - \\hat{y})^2}{\\delta W_{(2)}}$$\n",
    "\n",
    "Product rule to get rid of the 1/2:\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = (y - \\hat{y})$$\n",
    "\n",
    "Now, lets's perform the chain rule. Y is our inputs, this is our constant and will be zero. $ \\hat{y} $ is our fitted equation with our activation function, weights, and inputs. Let's break that down with chain rule.\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = (y - \\hat{y}) * -(\\dfrac{\\delta \\hat{y}}{\\delta W_{(2)}}) $$\n",
    "\n",
    "$$\\dfrac{\\delta J}{\\delta W_{(2)}} = -(y - \\hat{y})(\\dfrac{\\delta \\hat{y}}{\\delta W_{(2)}})$$\n",
    "\n",
    "Since $ \\hat{y} $ is our total weights combined with our activation function f: $ \\hat{y} = f(z_{(3)}) $, we need to perform chain rule again and breakdown $ \\dfrac{\\delta \\hat{y}}{\\delta W_{(2)}} $.\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = -(y - \\hat{y}) \\dfrac{\\delta \\hat{y}}{\\delta z_{(3)}} \\dfrac{\\delta z_{3}}{\\delta W_{(2)}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the rate of change of $\\hat{y}$ with respect to $ z_{(3)} $, we need to differentiate the activation function in our case, we are going to differentiate the sigmoid function.\n",
    "$$ f(z) = \\dfrac{1}{1 + e^{-2}} $$\n",
    "$$ f'(z) = \\dfrac{e^{-2}}{(1+e^{-2})^2} $$\n",
    "\n",
    "$$ \\dfrac{\\delta \\hat{y}}{\\delta W_{(2)}} = f'(z_{(3)}) = \\dfrac{e^{-2}}{(1+e^{-2})^2} $$\n",
    "\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} = -(y - \\hat{y}) f'(z_{(3)}) \\dfrac{\\delta z_{3}}{\\delta W_{(2)}}$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last part of the equation, $ \\dfrac{\\delta z_{3}}{\\delta W_{(2)}} $ represents the change of z3 (outputs) in respect to weights from the second layer. This is the activity of each synapse. In other words,  $ z_{(3)} = a_{(2)}w_{(2)} $ or the total of all outputs from hidden layer times the weights related to the outputs. It's also important to note that $ a_{(2)} $ is the slope. ... we can take care of the summation???\n",
    "\n",
    "Backpropagating error:\n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    y_1 - \\hat{y}_1 \\\\\n",
    "    y_2 - \\hat{y}_2 \\\\\n",
    "    y_3 - \\hat{y}_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "x\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    f'(z_{(3)})_1 \\\\\n",
    "    f'(z_{(3)})_2 \\\\\n",
    "    f'(z_{(3)})_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "=\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\delta_1 \\\\\n",
    "    \\delta_2 \\\\\n",
    "    \\delta_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "=\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\delta_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Kind of:\n",
    "$$\\delta_3 = -(y_3 - \\hat{y}_3) * f'(z_{(3)})_1 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through multiplying our scalar with $ \\dfrac{\\delta z_{3}}{\\delta W_{(2)}} $ we not only do we get our hidden layer and output size, but this performs our summation for us.\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(2)}} =  (a^2)^T * \\delta_{(3)}$$ \n",
    "\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    x_{11}^{(2)} & x_{21}^{(2)} & x_{31}^{(2)} \\\\\n",
    "    x_{12}^{(2)} & x_{22}^{(2)} & x_{32}^{(2)} \\\\\n",
    "    x_{13}^{(2)} & x_{23}^{(2)} & x_{33}^{(2)} \n",
    "\\end{bmatrix}\n",
    "$\n",
    "*\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    \\delta_3 \\\\\n",
    "    \\delta_3 \\\\\n",
    "    \\delta_3 \\\\\n",
    "\\end{bmatrix}\n",
    "$ \n",
    "=\n",
    "$\n",
    "\\begin{bmatrix}\n",
    "    x_{11}^{(2)}\\delta_3 & x_{21}^{(2)}\\delta_3 & x_{31}^{(2)}\\delta_3 \\\\\n",
    "    x_{12}^{(2)}\\delta_3 & x_{22}^{(2)}\\delta_3 & x_{32}^{(2)}\\delta_3 \\\\\n",
    "    x_{13}^{(2)}\\delta_3 & x_{23}^{(2)}\\delta_3 & x_{33}^{(2)}\\delta_3 \n",
    "\\end{bmatrix}\n",
    "$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deriving DJ d1\n",
    "Lets begin deriving\n",
    "$$ \\dfrac{\\delta J}{\\delta W_{(1)}} = -(y - \\hat{y}) \\dfrac{\\delta \\hat{y}}{\\delta z_{(3)}} \\dfrac{\\delta z_{3}}{\\delta W_{(1)}}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
